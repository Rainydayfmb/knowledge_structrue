<!-- TOC -->

- [1. dubbo](#1-dubbo)
    - [1.1. dubbo的核心功能](#11-dubbo的核心功能)
    - [1.2. dubbo的组件角色](#12-dubbo的组件角色)
    - [1.3. dubbo的总体框架](#13-dubbo的总体框架)
        - [1.3.1. dubbo的各层与节点之间的关系](#131-dubbo的各层与节点之间的关系)
        - [1.3.2. 服务调用流程](#132-服务调用流程)
        - [1.3.3. 注册/注销服务](#133-注册注销服务)
        - [1.3.4. 订阅/取消服务](#134-订阅取消服务)
        - [1.3.5. dubbo的工作流程](#135-dubbo的工作流程)
    - [1.4. dubbo 支持哪些通信协议？支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？](#14-dubbo-支持哪些通信协议支持哪些序列化协议说一下-hessian-的数据结构pb-知道吗为什么-pb-的效率是最高的)
        - [1.4.1. dubbo支持不同的通信协议](#141-dubbo支持不同的通信协议)
    - [1.5. dubbo通信模型](#15-dubbo通信模型)
        - [1.5.1. BIO通信](#151-bio通信)
            - [1.5.1.1. BIO通信缺陷](#1511-bio通信缺陷)
        - [1.5.2. NIO单一长连接实现分析](#152-nio单一长连接实现分析)
            - [1.5.2.1. 长连接宏观简介](#1521-长连接宏观简介)
            - [1.5.2.2. 使用NIO设计RPC调用分析](#1522-使用nio设计rpc调用分析)
        - [1.5.3. 如何基于dubbo进行服务治理、服务降级、失败重试以及超时重试](#153-如何基于dubbo进行服务治理服务降级失败重试以及超时重试)
            - [1.5.3.1. 服务治理](#1531-服务治理)
            - [1.5.3.2. 服务降级](#1532-服务降级)
            - [1.5.3.3. 失败重试和超时重试](#1533-失败重试和超时重试)
            - [1.5.3.4. 分布式接口的顺序性如何保证](#1534-分布式接口的顺序性如何保证)

<!-- /TOC -->
# 1. dubbo
而对于远程调用如果没有分布式的需求，其实是不需要用这么重的框架，只有在分布式的时候，才有Dubbo这样的分布式服务框架的需求，说白了就是个远程服务调用的分布式框架，**其重点在于分布式的治理。**

## 1.1. dubbo的核心功能
1.**Remoting:远程通讯**，提供对多种NIO框架抽象封装，包括“同步转异步”和“请求-响应”模式的信息交换方式。
2.**Cluster: 服务框架**，提供基于接口方法的透明远程过程调用，包括多协议支持，以及软负载均衡，失败容错，地址路由，动态配置等集群支持。
3.**Registry: 服务注册中心**，基于注册中心目录服务，使服务消费方能动态的查找服务提供方，使地址透明，使服务提供方可以平滑增加或减少机器。

## 1.2. dubbo的组件角色
![avatar](https://user-gold-cdn.xitu.io/2018/3/20/16241d6d49a76c43?imageView2/0/w/1280/h/960/ignore-error/1)

**调用关系说明：**
1.服务容器Container负责启动，加载，运行服务提供者。
2.服务提供者Provider在启动时，向注册中心注册自己提供的服务。
3.服务消费者Consumer在启动时，向注册中心订阅自己所需的服务。
4.注册中心Registry返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。
5.服务消费者Consumer，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。
6.服务消费者Consumer和提供者Provider，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心Monitor。
![avatar](https://user-gold-cdn.xitu.io/2018/3/20/16241d6d49d9ccf0?imageView2/0/w/1280/h/960/ignore-error/1)

## 1.3. dubbo的总体框架
ubbo最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。所以，我们横向以分层的方式来看下Dubbo的架构，如图所示：
![avatar](https://user-gold-cdn.xitu.io/2018/3/20/16241d6d49c35881?imageView2/0/w/1280/h/960/ignore-error/1)

Dubbo框架设计一共划分了10个层，而最上面的Service层是留给实际想要使用Dubbo开发分布式服务的开发者实现业务逻辑的接口层。图中左边淡蓝背景的为服务消费方使用的接口，右边淡绿色背景的为服务提供方使用的接口， 位于中轴线上的为双方都用到的接口。

- 配置层（Config）：对外配置接口，以ServiceConfig和ReferenceConfig为中心，可以直接new配置类，也可以通过Spring解析配置生成配置类。
- 服务代理层（Proxy）：服务接口透明代理，生成服务的客户端Stub和服务器端Skeleton，以ServiceProxy为中心，扩展接口为ProxyFactory。
- 服务注册层（Registry）：封装服务地址的注册与发现，以服务URL为中心，扩展接口为RegistryFactory、Registry和RegistryService。可能没有服务注册中心，此时服务提供方直接暴露服务。
- 集群层（Cluster）：封装多个提供者的路由及负载均衡，并桥接注册中心，以Invoker为中心，扩展接口为Cluster、Directory、Router和LoadBalance。将多个服务提供方组合为一个服务提供方，实现对服务消费方来透明，只需要与一个服务提供方进行交互。
- 监控层（Monitor）：RPC调用次数和调用时间监控，以Statistics为中心，扩展接口为MonitorFactory、Monitor和MonitorService。
- 远程调用层（Protocol）：封将RPC调用，以Invocation和Result为中心，扩展接口为Protocol、Invoker和Exporter。Protocol是服务域，它是Invoker暴露和引用的主功能入口，它负责Invoker的生命周期管理。Invoker是实体域，它是Dubbo的核心模型，其它模型都向它靠扰，或转换成它，它代表一个可执行体，可向它发起invoke调用，它有可能是一个本地的实现，也可能是一个远程的实现，也可能一个集群实现。
- 信息交换层（Exchange）：封装请求响应模式，同步转异步，以Request和Response为中心，扩展接口为Exchanger、ExchangeChannel、ExchangeClient和ExchangeServer。
- 网络传输层（Transport）：抽象mina和netty为统一接口，以Message为中心，扩展接口为Channel、Transporter、Client、Server和Codec。
- 数据序列化层（Serialize）：可复用的一些工具，扩展接口为Serialization、 ObjectInput、ObjectOutput和ThreadPool。

从上图可以看出，Dubbo对于服务提供方和服务消费方，从框架的10层中分别提供了各自需要关心和扩展的接口，构建整个服务生态系统（服务提供方和服务消费方本身就是一个以服务为中心的）。

### 1.3.1. dubbo的各层与节点之间的关系
*根据官方提供的，对于上述各层之间关系的描述，如下所示：
1.在RPC中，Protocol是核心层，也就是只要有Protocol + Invoker + Exporter就可以完成非透明的RPC调用，然后在Invoker的主过程上Filter拦截点。
2.图中的Consumer和Provider是抽象概念，只是想让看图者更直观的了解哪些类分属于客户端与服务器端，不用Client和Server的原因是Dubbo在很多场景下都使用Provider、Consumer、Registry、Monitor划分逻辑拓普节点，保持统一概念。
3.而Cluster是外围概念，所以Cluster的目的是将多个Invoker伪装成一个Invoker，这样其它人只要关注Protocol层Invoker即可，加上Cluster或者去掉Cluster对其它层都不会造成影响，因为只有一个提供者时，是不需要Cluster的。
[dubbo之cluster层](https://segmentfault.com/a/1190000017089603)
![avatar](https://upload-images.jianshu.io/upload_images/12016719-16d235b1ddf7b489.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/550)
**dubbo各节点关系：**
- 这里的Invoker是Provider的一个可调用Service的抽象，Invoker封装了Provider地址及Service接口信息；
- Directory代表多个Invoker，可以把它看成List，但与List不同的是，它的值可能是动态变化的，比如注册中心推送变更；
- Cluster将Directory中的多个Invoker伪装成一个 Invoker，对上层透明，伪装过程包含了容错逻辑，调用失败后，重试另一个；
- Router负责从多个Invoker中按路由规则选出子集，比如读写分离，应用隔离等；
- LoadBalance负责从多个Invoker中选出具体的一个用于本次调用，选的过程包含了负载均衡算法，调用失败后，需要重选；
- Cluster经过目录，路由，负载均衡获取到一个可用的Invoker，交给上层调用。

4.Proxy层封装了所有接口的透明化代理，而在其它层都以Invoker为中心，只有到了暴露给用户使用时，才用Proxy将Invoker转成接口，或将接口实现转成Invoker，也就是去掉Proxy层RPC是可以Run的，只是不那么透明，不那么看起来像调本地服务一样调远程服务。

5.而Remoting实现是Dubbo协议的实现，如果你选择RMI协议，整个Remoting都不会用上，Remoting内部再划为Transport传输层和Exchange信息交换层，Transport层只负责单向消息传输，是对Mina、Netty、Grizzly的抽象，它也可以扩展UDP传输，而Exchange层是在传输层之上封装了Request-Response语义。
6.Registry和Monitor实际上不算一层，而是一个独立的节点，只是为了全局概览，用层的方式画在一起。

### 1.3.2. 服务调用流程
![avatar](https://user-gold-cdn.xitu.io/2018/3/20/16241d6d49cc5515?imageView2/0/w/1280/h/960/ignore-error/1)

### 1.3.3. 注册/注销服务
![avatar](https://user-gold-cdn.xitu.io/2018/3/20/16241d6d49b32422?imageView2/0/w/1280/h/960/ignore-error/1)

### 1.3.4. 订阅/取消服务
![avatar](https://user-gold-cdn.xitu.io/2018/3/20/16241d6d49a8c8d2?imageView2/0/w/1280/h/960/ignore-error/1)

### 1.3.5. dubbo的工作流程
- 第一步：provider 向注册中心去注册
- 第二步：consumer 从注册中心订阅服务，注册中心会通知 consumer 注册好的服务
- 第三步：consumer 调用 provider
- 第四步：consumer 和 provider 都异步通知监控中心
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/dubbo-operating-principle.png)

从上到下：服 配 代 注 聚 监 协 交 传 序

## 1.4. dubbo 支持哪些通信协议？支持哪些序列化协议？说一下 Hessian 的数据结构？PB 知道吗？为什么 PB 的效率是最高的？

**序列化**:就是把数据结构或者是一些对象，转换为二进制串的过程。
**反序列化**：是将在序列化过程中所生成的二进制串转换成数据结构或者对象的过程。
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/serialize-deserialize.png)

### 1.4.1. dubbo支持不同的通信协议
- dubbo
认就是走 dubbo 协议，单一长连接，进行的是 NIO 异步通信，基于 hessian 作为序列化协议。使用的场景是：传输数据量小（每次请求在 100kb 以内），但是并发量很高。
为了要支持高并发场景，一般是服务提供者就几台机器，但是服务消费者有上百台，可能每天调用量达到上亿次！此时用长连接是最合适的，就是跟每个服务消费者维持一个长连接就可以，可能总共就 100 个连接。然后后面直接基于长连接 NIO 异步通信，可以支撑高并发请求。

**长连接**，通俗点说，就是建立连接过后可以持续发送请求，无须再建立连接。
**短连接**，每次要发送请求之前，需要先重新建立一次连接。

## 1.5. dubbo通信模型
[nio单一长连接--dubbo通信模型实现](https://www.jianshu.com/p/13bef2795c44)
### 1.5.1. BIO通信
BIO的完整连接示意图如下：
![avatar](https://upload-images.jianshu.io/upload_images/3727888-76b1222bc2d3a0c3.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/920/format/webp)

#### 1.5.1.1. BIO通信缺陷
**缺陷一、IO阻塞**
可以看出，在BIO中，除了建立连接比较耗时之外，在客户端将数据传输到服务端之前，服务端的IO（输入流）阻塞，然后在服务端将返回值传输回来之前，客户端的IO（输入流）阻塞。**也就是说在一次RPC调用开始到完成之前，这个连接一直被此次调用所占用，但是实际上这次调用中，真正需要网络连接的只有中间的数据传输过程，在客户端写出和服务端读取并执行远端方法这两个时间点，其实网络连接是空闲的**。这就是BIO连接中浪费了网络资源的地方。

**缺陷二、大量连接**
1. 由于BIO的IO阻塞，导致每次RPC调用会占用一个连接。而正因为如此，为了减少频繁创建连接消耗的时间，引入了连接池（此处的连接池指普通的HTTP连接池，非异步连接池）的概念，连接池解决了频繁创建连接的资源消耗，但是没有解决根本性的阻塞问题。而且在服务消费者（客户端）数量远大于服务提供者（服务端）数量的时候，会导致服务提供者建立了大量的连接，而本身由于硬件资源的限制，单机最大连接数是有限的（这个限制以前是1w，也就是以前的C10K问题，据说近几年已经提升至50万个连接），所以在服务消费者过多，而服务提供者数量过少的情况下，服务提供者有因为过多的连接而被拖垮的风险（这需要极大的并发数，每秒上百万次的调用）。当然，要解决这个问题，增加机器，从而增加服务提供者数量是可以解决的，但是没有充分利用单机性能。
2. 建立大量连接的另一个弊端，是操作系统频繁的线程上下文切换，因为连接数过多，线程切换频繁，会消耗大量的资源，而且这些切换可能不是必要的。比如当前建立了大量的连接，可能大部分处于阻塞状态，根本没有挨个挨个切换的必要。但是因为操作系统任务调度时并不会忽略阻塞状态的线程，所以造成浪费。

### 1.5.2. NIO单一长连接实现分析
#### 1.5.2.1. 长连接宏观简介
![avatar](https://upload-images.jianshu.io/upload_images/3727888-2632e469660f2db2.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)
NIO由三种角色组成，Selector、SocketChannel、Buffer。

*SocketChannel*: 相当于BIO中的Socket，分为SocketChannel和ServerSocketChannel两种，是真正建立连接并传输数据的管道。这个管道不同于BIO的Socket的点就是，这个管道可以被多个线程共用，线程A使用这个管道写出数据了之后，线程B还可以使用这个管道写出数据，不再被某一次调用所独占。所以就可以不需要像BIO一样建立那么多的连接，一个客户端的一个连接就够了（当然，实际应用中因为机器都是多核，实际上建立核数个连接个人感觉是比较好的）。

*Buffer*:是用来与SocketChannel互通数据的对象，本质上是一块内存区域。SocketChannel是不支持直接读写数据的，所有的读写操作必须通过Buffer来实现。值得一提的是，我们经常说在JVM中有的时候会使用虚拟机之外的内存，说的就是NIO中的Buffer，在分配内存的时候可以选择使用虚拟机外内存，减少数据的复制。

*Selector*:是用来监控SocketChannel的事件的，其实是实现非阻塞的关键。NIO是基于事件的，将BIO中的流式传输改为了事件机制。BIO中，一个连接拥有一个输入／输出流，只要数据传输完成，流就可以读取数据。在NIO中，Selector定义了四种事件，OP_READ、OP_WRITE、OP_CONNECT、OP_ACCEPT。当服务端或者客户端收到写入完成的一次数据时，会触发OP_READ事件，此时可以从连接中读取数据。同理，当可以往连接中写入数据的时候，触发OP_WRITE事件（但是一般情况下这个事件没有必要，因为连接一般都是可写的）。客户端与服务端建立连接的时候，客户端会收到OP_CONNECT事件，而服务端会触发OP_ACCEPT事件。通过这一系列事件将数据的发送与读写解耦，实现异步调用。将一个SocketChannel+一个事件绑定在一个Selector上，Selector本质上是轮询每一个SocketChannel，如果没有事件触发，那么线程阻塞，如果有事件触发，返回对应的SocketChannel，以便进行后续的处理。

#### 1.5.2.2. 使用NIO设计RPC调用分析
前面提到，由于NIO的SocketChannel是非阻塞的，所以不再需要连接池，使用一个连接就够了。
![avatar](https://upload-images.jianshu.io/upload_images/3727888-2a3e097144cd7a11?imageMogr2/auto-orient/strip|imageView2/2/w/1072/format/webp)

但是如果真的使用NIO来进行RPC调用的话，会有数据和调用方对应不上的问题，如下图：
![avatar](https://upload-images.jianshu.io/upload_images/3727888-a93021a1bbf552f8.png?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)
每次调用的返回值必须与调用方对应上，为此，Dubbo的设计是给每个请求设计一个请求id，在发送请求与发送返回值时都带上这个id。详细思路如下图：
![avatar](https://upload-images.jianshu.io/upload_images/3727888-098e4b3fb2b736f2.jpg?imageMogr2/auto-orient/strip|imageView2/2/w/1200/format/webp)
业务线程在发出请求之前，需要存储一个请求对象，同时挂起相应的业务线程（挂起不会被任务调度，所以不存在线程切换消耗），这个请求对象包含了此次请求的id，然后在获取服务端返回的数据的时候，解析出这个id，通过这个id取出请求对象，并唤醒对应的线程。

### 1.5.3. 如何基于dubbo进行服务治理、服务降级、失败重试以及超时重试
#### 1.5.3.1. 服务治理
1.调用链路自动生成
一个大型的分布式系统，或者说是用现在流行的微服务框架来说，分布式系统由大量的服务组成。那么这些服务之间是如何相互调用的？调用链路是什么？
需要基于dubbo做的分布式系统中，对各个服务之间的调用自动记录下来，然后自动将各个服务之间的依赖关系和调用链路生成出来，做成一张图，现实出来，这样大家才可以看到。
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/dubbo-service-invoke-road.png)

2.服务访问压力以及时长统计
需要自动统计各个接口和服务之间的调用次数以及访问延时，而且要分成两个级别
- 一个级别是接口粒度，就是每个服务的每个接口每天被调用多少次，TP50h/TP90/TP99，三个档次的请求延时分别是多少；
**tp的含义**：指在一段时间内，统计该方法每次调用所消耗的时间，并将这些时间按从小到大的顺序进行排序，并取出结果为：总次数*指标数=对应TP指标的序号，再根据序号取出对应排序好的时间，即TP指标值。
- 第二个级别从源头的入口开始，一个完整的请求链路经过几十个服务之后，完成一次请求，每天全链路走多少次，全链路的请求延时的TP50/TP90/TP99分别是多少。

这些东西都搞定了之后，后面才可以看到当前系统的主要压力在哪，如何来进行优化和扩容。

3.其他
- 服务分层（避免循环依赖）
- 调用链路失败监控和报警
- 服务鉴权
- 每个服务的可用性监控（接口调用的成功率？几个9？99.99%,99.9%,99%）

#### 1.5.3.2. 服务降级
比如说服务A调用服务B,结果服务B挂了，服务A重试几次后调用服务B还是不行，那么直接降级，走一个备用的逻辑，给客户返回响应。

#### 1.5.3.3. 失败重试和超时重试
所谓失败重试，就是 consumer 调用 provider 要是失败了，比如抛异常了，此时应该是可以重试的，或者调用超时了也可以重试。

//todo dubbo的filter 涉及到dubbo的责任链模式
[参考](https://www.jianshu.com/p/c5ebe3e08161)

#### 1.5.3.4. 分布式接口的顺序性如何保证
首先，个人建议是，你们从业务逻辑上设计的这个系统最好不要有这种顺序性的保证，因为一旦引入顺序性保证，比如适应分布式锁，会导致系统复杂度上升，而且会带来效率低下，热点数据压力过大的问题。

下面我给个我们用过的方案吧，简单来说，首先你得用 dubbo 的一致性 hash 负载均衡策略，将比如某一个订单 id 对应的请求都给分发到某个机器上去，接着就是在那个机器上，因为可能还是多线程并发执行的，你可能得立即将某个订单 id 对应的请求扔一个内存队列里去，强制排队，这样来确保他们的顺序性。
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/distributed-system-request-sequence.png)

但是这样引发的后续问题就很多，比如说要是某个订单对应的请求特别多，造成某台机器成热点怎么办？解决这些问题又要开启后续一连串的复杂技术方案......曾经这类问题弄的我们头疼不已，所以，还是建议什么呢？

最好是比如说刚才那种，一个订单的插入和删除操作，能不能合并成一个操作，就是一个删除，或者是其它什么，避免这种问题的产生。