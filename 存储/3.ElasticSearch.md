<!-- TOC -->

- [1. Elasticsearch](#1-elasticsearch)
    - [1.1. ELK stack](#11-elk-stack)
    - [1.2. Elasticsearch认知](#12-elasticsearch认知)
        - [1.2.1. Elasticsearch 比传统的关系性数据库(oracle,mysqls)非关系型数据库（mongodb）对比](#121-elasticsearch-比传统的关系性数据库oraclemysqls非关系型数据库mongodb对比)
    - [1.3. Logstash认知](#13-logstash认知)
    - [1.4. kibana认知](#14-kibana认知)
    - [1.5. Beats 认知](#15-beats-认知)
    - [1.6. ELK stack](#16-elk-stack)
        - [1.6.1. 场景一：使用 ES 作为业务系统的后端。](#161-场景一使用-es-作为业务系统的后端)
        - [1.6.2. 场景二：在原有系统中增加 ES、Logstash、Kibana等。](#162-场景二在原有系统中增加-eslogstashkibana等)
        - [1.6.3. 场景三：使用 ELK Stack 结合现有工具对外提供服务。](#163-场景三使用-elk-stack-结合现有工具对外提供服务)
        - [1.6.4. 场景四：其他综合业务场景](#164-场景四其他综合业务场景)
    - [1.7. 大量数据的写入和读取解决方案](#17-大量数据的写入和读取解决方案)
    - [1.8. ES的定义](#18-es的定义)
    - [1.9. ES的核心概念](#19-es的核心概念)
    - [1.10. ES要解决的问题](#110-es要解决的问题)
    - [1.11. ES的工作原理](#111-es的工作原理)
        - [1.11.1. ES的写入过程](#1111-es的写入过程)
        - [1.11.2. ES的读数据过程](#1112-es的读数据过程)
        - [1.11.3. es 搜索数据过程](#1113-es-搜索数据过程)
        - [1.11.4. 更新操作](#1114-更新操作)
        - [1.11.5. 更新和删除](#1115-更新和删除)
        - [1.11.6. 写数据底层原理](#1116-写数据底层原理)
        - [1.11.7. 倒排索引](#1117-倒排索引)
        - [1.11.8. es实现高可用的原理](#1118-es实现高可用的原理)
    - [1.12. ES特点和优势](#112-es特点和优势)
        - [1.12.1. ES在那些场景下面可以代替传动的db](#1121-es在那些场景下面可以代替传动的db)
        - [1.12.2. es 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？](#1122-es-在数据量很大的情况下数十亿级别如何提高查询效率啊)
            - [1.12.2.1. 性能优化的杀手锏——filesystem cache](#11221-性能优化的杀手锏filesystem-cache)
            - [1.12.2.2. 数据预热](#11222-数据预热)
            - [1.12.2.3. 冷热分离](#11223-冷热分离)
            - [1.12.2.4. document 模型设计](#11224-document-模型设计)
            - [1.12.2.5. 分页性能优化](#11225-分页性能优化)
        - [1.12.3. es 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？](#1123-es-生产集群的部署架构是什么每个索引的数据量大概有多少每个索引大概有多少个分片)

<!-- /TOC -->
# 1. Elasticsearch
[参考](https://blog.csdn.net/laoyang360/article/details/79293493?spm=a2c4e.10696291.0.0.6c5019a4RdoGp6)
## 1.1. ELK stack
ELK stack由最早期的最核心的Elasticsearch ,集合Logstash,kibana,beats等发展而来，形成ELK stack体系。如下图所示
![avatar](https://yqfile.alicdn.com/7cc0ee14e7d1098ccf448cfff37309ef475c6221.png)

## 1.2. Elasticsearch认知
Elasticsearch 为开源的，分布式的，基于restful API、支持PB甚至更改数量级的搜索引擎工具。
相对于mysql,给出如下的对应关系表会更好的理解一些：
![avatar](https://yqfile.alicdn.com/e164c17310623ce6dfdf169e0c88368772ff343f.png)
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/es-index-type-mapping-document-field.png)

### 1.2.1. Elasticsearch 比传统的关系性数据库(oracle,mysqls)非关系型数据库（mongodb）对比
如下是传统的关系型数据库（如Oracle、MySQL）、非关系型的数据库（如 Mongo）所做不到的：
1.传统的关系型数据库虽然能支持类型“like 待检索词”模糊语句匹配，但无法进行全文检索（分词检索）。
2.非关系型数据库 Mongo 虽能进行简单的全文检索，但对中文支持的不好、数据量大性能会有问题，这点是在实际应用中总结出的。
**mongodb的全文检索原理**
[mongodb的全文检索依靠的是标签和索引](http://www.voidcn.com/article/p-zlcjehfe-de.html)

## 1.3. Logstash认知
[logstash最佳实践](https://doc.yonyoucloud.com/doc/logstash-best-practice-cn/input/file.html)
可以把 Logstash 理解成流入、流出 Elasticsearch 的传送带。

支持：不同类型的数据或实施数据流经过 Logstash 写入 ES 或者从 ES 中读出写入文件或对应的实施数据流。包括但不限于：
- 本地或远程文件；
- Kafka 实时数据流——核心插件有 logstashinputkafka/logstashoutputkafka；
- MySQL、Oracle 等关系型数据库——核心插件有 logstashinputjdbc/logstashouputjdbc；
- Mongo 非关系型数据库——核心插件有 logstashinputmongo/logstashoutputmongo；
- Redis 数据流；

## 1.4. kibana认知
Kibana 是 ES 大数据的图形化展示工具。集成了 DSL 命令行查看、数据处理插件、继承了 x-pack（收费）安全管理插件等。

## 1.5. Beats 认知

Beats 是一个开源的用来构建轻量级数据汇集的平台，可用于将各种类型的数据发送至 Elasticsearch 与 Logstash。

## 1.6. ELK stack
### 1.6.1. 场景一：使用 ES 作为业务系统的后端。
此时，ES 的作用类似传统业务系统中的 MySQL、PostgreSQL、Oracle 或者 Mongo 等的基础关系型数据库或非关系型数据库的作用。

我们举例说明。使用 ES 对基础文档进行检索操作，如将传统的 word 文档、PDF 文档、PPT 文档等通过 Openoffice 或者 pdf2htmlEX 工具转换为 HTML，再将 HTML 以JSON 串的形式录入到 ES，以对外提供检索服务。

### 1.6.2. 场景二：在原有系统中增加 ES、Logstash、Kibana等。

原有的业务系统中存在 MySQL、Oracle、Mongo 等基础数据，但想实现全文检索服务，就在原有业务系统基础的加上一层 ELK。

举例一，将原有系统中 MySQL 中的数据通过 logstashinputjdbc 插件导入到 ES 中，并通过 Kibana 进行图形化展示。

举例二，将原有存储在 Hadoop HDFS 中的数据导入到 ES 中，对外提供检索服务。

### 1.6.3. 场景三：使用 ELK Stack 结合现有工具对外提供服务。

举例一，日志检索系统。将各种类型的日志通过 Logstash 导入 ES 中，通过 Kibana 或者 Grafana 对外提供可视化展示。

举例二，通过 Flume 等将数据导入 ES 中，通过 ES 对外提供全文检索服务。

### 1.6.4. 场景四：其他综合业务场景

主要借助 ES 强大的全文检索功能实现，如分页查询、各类数据结果的聚合分析、图形化展示（饼图、线框图、曲线图等）。

举例说明，像那些结合实际业务的场景，如安防领域、金融领域、监控领域等的综合应用。

## 1.7. 大量数据的写入和读取解决方案
1、存储数据时按有序存储；
2、将数据和索引分离；
3、压缩数据；

## 1.8. ES的定义
ES=elaticsearch简写， Elasticsearch是一个开源的高扩展的分布式全文检索引擎，它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。
Elasticsearch也使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，但是它的目的是通过简单的RESTful API来隐藏Lucene的复杂性，从而让全文搜索变得简单。
**数据模型**
![avatar](https://upload-images.jianshu.io/upload_images/1604849-f0256300e34c6ccf.png?imageMogr2/auto-orient/strip|imageView2/2/w/973)

## 1.9. ES的核心概念
1）Cluster：集群。
ES可以作为一个独立的单个搜索服务器。不过，为了处理大型数据集，实现容错和高可用性，ES可以运行在许多互相合作的服务器上。这些服务器的集合称为集群。

2）Node：节点。
形成集群的每个服务器称为节点。

3）Shard：分片。
当有大量的文档时，由于内存的限制、磁盘处理能力不足、无法足够快的响应客户端的请求等，一个节点可能不够。这种情况下，数据可以分为较小的分片。每个分片放到不同的服务器上。
当你查询的索引分布在多个分片上时，ES会把查询发送给每个相关的分片，并将结果组合在一起，而应用程序并不知道分片的存在。即：这个过程对用户来说是透明的。

4）Replia：副本。
为提高查询吞吐量或实现高可用性，可以使用分片副本。
副本是一个分片的精确复制，每个分片可以有零个或多个副本。ES中可以有许多相同的分片，其中之一被选择更改索引操作，这种特殊的分片称为主分片。
当主分片丢失时，如：该分片所在的数据不可用时，集群将副本提升为新的主分片。
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/es-cluster.png)
你搞一个索引，这个索引可以拆分成多个 shard，每个 shard 存储部分数据。拆分多个 shard 是有好处的:
**一是支持横向扩展**，比如你数据量是 3T，3 个 shard，每个 shard 就 1T 的数据，若现在数据量增加到 4T，怎么扩展，很简单，重新建一个有 4 个 shard 的索引，将数据导进去；
**二是提高性能**，数据分布在多个 shard，即多台服务器上，所有的操作，都会在多台机器上并行分布式执行，提高了吞吐量和性能。

接着就是这个 shard 的数据实际是有多个备份，就是说每个 shard 都有一个 primary shard，负责写入数据，但是还有几个 replica shard。primary shard 写入数据之后，会将数据同步到其他几个 replica shard 上去。
通过这个 replica 的方案，每个 shard 的数据都有多个备份，如果某个机器宕机了，没关系啊，还有别的数据副本在别的机器上呢。高可用了吧。


## 1.10. ES要解决的问题
1）检索相关数据；
2）返回统计结果；
3）速度要快。

## 1.11. ES的工作原理
当ElasticSearch的节点启动后，它会利用多播(multicast)(或者单播，如果用户更改了配置)寻找集群中的其它节点，并与之建立连接。这个过程如下图所示：
![avatar](https://img-blog.csdn.net/20160818205953345)

### 1.11.1. ES的写入过程
- 客户端选择一个 node 发送请求过去，这个 node 就是 coordinating node（协调节点）。
- coordinating node 对 document 进行路由，将请求转发给对应的 node（有 primary shard）。
- 实际的 node 上的 primary shard 处理请求，然后将数据同步到 replica node。
- coordinating node 如果发现 primary node 和所有 replica node 都搞定之后，就返回响应结果给客户端。
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/es-write.png)

### 1.11.2. ES的读数据过程
可以通过 doc id 来查询，会根据 doc id 进行 hash，判断出来当时把 doc id 分配到了哪个 shard 上面去，从那个 shard 去查询。

- 客户端发送请求到任意一个 node，成为 coordinate node。
- coordinate node 对 doc id 进行哈希路由，将请求转发到对应的 node，此时会使用 round-robin 随机轮询算法，在 primary shard 以及其所有 replica 中随机选择一个，让读请求负载均衡。
- 接收请求的 node 返回 document 给 coordinate node。
- coordinate node 返回 document 给客户端。

### 1.11.3. es 搜索数据过程
es 最强大的是做全文检索，就是比如你有三条数据：
```java
java真好玩儿啊
java好难学啊
j2ee特别牛
```
你根据 java 关键词来搜索，将包含 java的 document 给搜索出来。es 就会给你返回：java真好玩儿啊，java好难学啊。
- 客户端发送请求到一个 coordinate node。
- 协调节点将搜索请求转发到所有的 shard 对应的 primary shard 或 replica shard，都可以。
- query phase：每个 shard 将自己的搜索结果（其实就是一些 doc id）返回给协调节点，由协调节点进行数据的合并、排序、分页等操作，产出最终结果。
- fetch phase：接着由协调节点根据 doc id 去各个节点上拉取实际的 document 数据，最终返回给客户端。

写请求是写入 primary shard，然后同步给所有的 replica shard；读请求可以从 primary shard 或 replica shard 读取，采用的是随机轮询算法。

### 1.11.4. 更新操作
更新操作其实就是先读然后写
![avatar](https://upload-images.jianshu.io/upload_images/1604849-c438f2d30c8c8fbd.png?imageMogr2/auto-orient/strip|imageView2/2/w/910/format/webp)
更新流程：

客户端将更新请求发给Node1
1.Node1根据文档ID(_id字段)计算出该文档属于分片shard0,而其主分片在Node上，于是将请求路由到Node3
2.Node3从p0读取文档，改变source字段的json内容，然后将修改后的数据在P0重新做索引。如果此时该文档被其他进程修改，那么将重新执行3步骤，这个过程如果超过retryon_confilct设置的重试次数，就放弃。
3.如果Node3成功更新了文档，它将并行的将新版本的文档同步到Node1 Node2的副本分片上重新建立索引，一旦所有的副本报告成功，Node3向被请求的Node1节点返回成功，然后Node1向client返回成功

### 1.11.5. 更新和删除
由于segments是不变的，所以文档不能从旧的segments中删除，也不能在旧的segments中更新来映射一个新的文档版本。取之的是，每一个提交点都会包含一个.del文件，列举了哪一个segment的哪一个文档已经被删除了。 当一个文档被”删除”了，它仅仅是在.del文件里被标记了一下。被”删除”的文档依旧可以被索引到，但是它将会在最终结果返回时被移除掉。

文档的更新同理：当文档更新时，旧版本的文档将会被标记为删除，新版本的文档在新的segment中建立索引。也许新旧版本的文档都会本检索到，但是旧版本的文档会在最终结果返回时被移除。


### 1.11.6. 写数据底层原理
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/es-write-detail.png)
[Elasticsearch基础整理](https://www.jianshu.com/p/e8226138485d)
- **write**:
先写入内存 buffer，在 buffer 里的时候数据是搜索不到的；同时将数据写入 translog 日志文件。
![avatar](https://upload-images.jianshu.io/upload_images/1604849-de89cebe9a0976da.png?imageMogr2/auto-orient/strip|imageView2/2/w/627/format/webp)
这时候还不能会被检索到，而数据必须被refresh到segment后才能被检索到。
- **refresh**:
默认情况下es每隔1s执行一次refresh,太耗性能，可以通过index.refresh_interval来修改这个刷新时间间隔。
整个refresh具体做了如下事情:
1.所有在内存缓冲区的文档被写入到一个新的segment中，但是没有调用fsync，因此数据有可能丢失,此时segment首先被写到内核的文件系统中缓存os cache中
2.segment被打开是的里面的文档能够被见检索到
3.清空内存缓冲区in-memory buffer，清空后如下图
![avatar](https://upload-images.jianshu.io/upload_images/1604849-5e6ef487fe33a943.png?imageMogr2/auto-orient/strip|imageView2/2/w/912/format/webp)
*补充*：
1.每隔 1 秒钟，es 将 buffer 中的数据写入一个新的 segment file，每秒钟会产生一个新的磁盘文件 segment file，这个 segment file 中就存储最近 1 秒内 buffer 中写入的数据。
2.但是如果 buffer 里面此时没有数据，那当然不会执行 refresh 操作，如果 buffer 里面有数据，默认 1 秒钟执行一次 refresh 操作，刷入一个新的 segment file 中。
3.操作系统里面，磁盘文件其实都有一个东西，叫做 os cache，即操作系统缓存，就是说数据写入磁盘文件之前，会先进入 os cache，先进入操作系统级别的一个内存缓存中去。只要 buffer 中的数据被 refresh 操作刷入 os cache中，这个数据就可以被搜索到了。
- **flush**:
随着translog文件越来越大时要考虑把内存中的数据刷新到磁盘中，这个过程叫flush
1.把所有在内存缓冲区中的文档写入到一个新的segment中
2.清空内存缓冲区
3.往磁盘里写入commit point信息
4.文件系统的page cache(segments) fsync到磁盘
5.删除旧的translog文件，因此此时内存中的segments已经写入到磁盘中,就不需要translog来保障数据安全了，flush后效果如下
![avatar](https://upload-images.jianshu.io/upload_images/1604849-34ea52e9cc4409e5.png?imageMogr2/auto-orient/strip|imageView2/2/w/582/format/webp)
- **segment合并**：
通过每隔一秒的自动刷新机制会创建一个新的segment，用不了多久就会有很多的segment。segment会消耗系统的文件句柄，内存，CPU时钟。最重要的是，每一次请求都会依次检查所有的segment。segment越多，检索就会越慢。
ES通过在后台merge这些segment的方式解决这个问题。小的segment merge到大的
这个过程也是那些被”删除”的文档真正被清除出文件系统的过程，因为被标记为删除的文档不会被拷贝到大的segment中。
![avatar](https://upload-images.jianshu.io/upload_images/1604849-ea07c8a178cdb69e.png?imageMogr2/auto-orient/strip|imageView2/2/w/861/format/webp)
1.当在建立索引过程中，refresh进程会创建新的segments然后打开他们以供索引。
2.merge进程会选择一些小的segments然后merge到一个大的segment中。这个过程不会打断检索和创建索引。一旦merge完成，旧的segments将被删除。
![avatar](https://upload-images.jianshu.io/upload_images/1604849-2cc44a497a148b1c.png?imageMogr2/auto-orient/strip|imageView2/2/w/799/format/webp)

### 1.11.7. 倒排索引
在搜索引擎中，每个文档都有一个对应的文档 ID，文档内容被表示为一系列关键词的集合。例如，文档 1 经过分词，提取了 20 个关键词，每个关键词都会记录它在文档中出现的次数和出现位置。

那么，倒排索引就是关键词到文档 ID 的映射，每个关键词都对应着一系列的文件，这些文件中都出现了关键词。

举个例子：
有以下文档：
DocId | Doc |
:-: | :-:
1   | 谷歌地图之父跳槽 Facebook
2   | 谷歌地图之父加盟 Facebook
3   | 谷歌地图创始人拉斯离开谷歌加盟 Facebook
4   | 谷歌地图之父跳槽 Facebook 与 Wave 项目取消有关
5   | 谷歌地图之父拉斯加盟社交网站 Facebook

对文档进行分词之后，得到以下倒排索引。
WordId | Word | DocIds
:-: | :-: | :-:
1	  |谷歌	 |1,2,3,4,5
2	  |地图	 |1,2,3,4,5
3	  |之父	 |1,2,4,5
4	  |跳槽	 |1,4
5	  |Facebook	|1,2,3,4,5
6	  |加盟	 |2,3,5
7	  |创始人 |3
8	  |拉斯	  |3,5
9	  |离开	  |3
10	|与	  |4
..	|..	   |..

### 1.11.8. es实现高可用的原理
es 集群多个节点，会自动选举一个节点为 master 节点，这个 master 节点其实就是干一些管理的工作的，比如维护索引元数据、负责切换 primary shard 和 replica shard 身份等。要是 master 节点宕机了，那么会重新选举一个节点为 master 节点。

如果是非 master节点宕机了，那么会由 master 节点，让那个宕机节点上的 primary shard 的身份转移到其他机器上的 replica shard。接着你要是修复了那个宕机机器，重启了之后，master 节点会控制将缺失的 replica shard 分配过去，同步后续修改的数据之类的，让集群恢复正常。

说得更简单一点，就是说如果某个非 master 节点宕机了。那么此节点上的 primary shard 不就没了。那好，master 会让 primary shard 对应的 replica shard（在其他机器上）切换为 primary shard。如果宕机的机器修复了，修复后的节点也不再是 primary shard，而是 replica shard。

其实上述就是 ElasticSearch 作为分布式搜索引擎最基本的一个架构设计。

5）全文检索。

全文检索就是对一篇文章进行索引，可以根据关键字搜索，类似于mysql里的like语句。
全文索引就是把内容根据词的意义进行分词，然后分别创建索引，例如”你们的激情是因为什么事情来的” 可能会被分词成：“你们“，”激情“，“什么事情“，”来“ 等token，这样当你搜索“你们” 或者 “激情” 都会把这句搜出来。

## 1.12. ES特点和优势
1）分布式实时文件存储，可将每一个字段存入索引，使其可以被检索到。
2）实时分析的分布式搜索引擎。
分布式：索引分拆成多个分片，每个分片可有零个或多个副本。集群中的每个数据节点都可承载一个或多个分片，并且协调和处理各种操作；
负载再平衡和路由在大多数情况下自动完成。
3）可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。也可以运行在单台PC上（已测试）
4）支持插件机制，分词插件、同步插件、Hadoop插件、可视化插件等。

### 1.12.1. ES在那些场景下面可以代替传动的db
1.个人以为Elasticsearch作为内部存储来说还是不错的，效率也基本能够满足，在某些方面替代传统DB也是可以的，前提是你的业务不对操作的事性务有特殊要求；而权限管理也不用那么细，因为ES的权限这块还不完善。
2.由于我们对ES的应用场景仅仅是在于对某段时间内的数据聚合操作，没有大量的单文档请求（比如通过userid来找到一个用户的文档，类似于NoSQL的应用场景），所以能否替代NoSQL还需要各位自己的测试。
3.如果让我选择的话，我会尝试使用ES来替代传统的NoSQL，因为它的横向扩展机制太方便了。


### 1.12.2. es 在数据量很大的情况下（数十亿级别）如何提高查询效率啊？
#### 1.12.2.1. 性能优化的杀手锏——filesystem cache
![avatar](https://raw.githubusercontent.com/doocs/advanced-java/master/images/es-search-process.png)
- 比如说你现在有一行数据。id,name,age .... 30 个字段。但是你现在搜索，只需要根据 id,name,age 三个字段来搜索。如果你傻乎乎往 es 里写入一行数据所有的字段，就会导致说 90% 的数据是不用来搜索的，结果硬是占据了 es 机器上的 filesystem cache 的空间，单条数据的数据量越大，就会导致 filesystem cahce 能缓存的数据就越少。其实，仅仅写入 es 中要用来检索的少数几个字段就可以了，比如说就写入 es id,name,age 三个字段，然后你可以把其他的字段数据存在 mysql/hbase 里，我们一般是建议用 es + hbase 这么一个架构。
- **hbase的特点是适用于海量数据的在线存储，就是对 hbase 可以写入海量数据，但是不要做复杂的搜索，做很简单的一些根据 id 或者范围进行查询的这么一个操作就可以了**。从 es 中根据 name 和 age 去搜索，拿到的结果可能就 20 个 doc id，然后根据 doc id 到 hbase 里去查询每个 doc id 对应的完整的数据，给查出来，再返回给前端。
- 写入 es 的数据最好小于等于，或者是略微大于 es 的 filesystem cache 的内存容量。然后你从 es 检索可能就花费 20ms，然后再根据 es 返回的 id 去 hbase 里查询，查 20 条数据，可能也就耗费个 30ms，可能你原来那么玩儿，1T 数据都放 es，会每次查询都是 5~10s，现在可能性能就会很高，每次查询就是 50ms。

#### 1.12.2.2. 数据预热
假如说，哪怕是你就按照上述的方案去做了，es 集群中每个机器写入的数据量还是超过了 filesystem cache 一倍，比如说你写入一台机器 60G 数据，结果 filesystem cache 就 30G，还是有 30G 数据留在了磁盘上。
其实可以做数据预热。
- 举个例子，拿微博来说，你可以把一些大V，平时看的人很多的数据，你自己提前后台搞个系统，每隔一会儿，自己的后台系统去搜索一下热数据，刷到 filesystem cache 里去，后面用户实际上来看这个热数据的时候，他们就是直接从内存里搜索了，很快。
- 或者是电商，你可以将平时查看最多的一些商品，比如说 iphone 8，热数据提前后台搞个程序，每隔 1 分钟自己主动访问一次，刷到 filesystem cache 里去。
- 对于那些你觉得比较热的、经常会有人访问的数据，最好做一个专门的缓存预热子系统，就是对热数据每隔一段时间，就提前访问一下，让数据进入 filesystem cache 里面去。这样下次别人访问的时候，性能一定会好很多。

#### 1.12.2.3. 冷热分离
- es 可以做类似于 mysql 的水平拆分，就是说将大量的访问很少、频率很低的数据，单独写一个索引，然后将访问很频繁的热数据单独写一个索引。最好是将冷数据写入一个索引中，然后热数据写入另外一个索引中，这样可以确保热数据在被预热之后，尽量都让他们留在 filesystem os cache 里，别让冷数据给冲刷掉。

#### 1.12.2.4. document 模型设计
- 对于 MySQL，我们经常有一些复杂的关联查询。在 es 里该怎么玩儿，es 里面的复杂的关联查询尽量别用，一旦用了性能一般都不太好。
- 最好是先在 Java 系统里就完成关联，将关联好的数据直接写入 es 中。搜索的时候，就不需要利用 es 的搜索语法来完成 join 之类的关联搜索了。
- document 模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作。es 能支持的操作就那么多，不要考虑用 es 做一些它不好操作的事情。如果真的有那种操作，尽量在 document 模型设计的时候，写入的时候就完成。另外对于一些太复杂的操作，比如 join/nested/parent-child 搜索都要尽量避免，性能都很差的。

#### 1.12.2.5. 分页性能优化
类似于 app 里的推荐商品不断下拉出来一页一页的

类似于微博中，下拉刷微博，刷出来一页一页的，你可以用 scroll api，关于如何使用，自行上网搜索。

- scroll 会一次性给你生成所有数据的一个快照，然后每次滑动向后翻页就是通过游标 scroll_id 移动，获取下一页下一页这样子，性能会比上面说的那种分页性能要高很多很多，基本上都是毫秒级的。
- 但是，唯一的一点就是，这个适合于那种类似微博下拉翻页的，不能随意跳到任何一页的场景。也就是说，你不能先进入第 10 页，然后去第 120 页，然后又回到第 58 页，不能随意乱跳页。所以现在很多产品，都是不允许你随意翻页的，app，也有一些网站，做的就是你只能往下拉，一页一页的翻。
- 初始化时必须指定 scroll 参数，告诉 es 要保存此次搜索的上下文多长时间。你需要确保用户不会持续不断翻页翻几个小时，否则可能因为超时而失败。
- 除了用 scroll api，你也可以用 search_after 来做，search_after 的思想是使用前一页的结果来帮助检索下一页的数据，显然，这种方式也不允许你随意翻页，你只能一页页往后翻。初始化时，需要使用一个唯一值的字段作为 sort 字段。

### 1.12.3. es 生产集群的部署架构是什么？每个索引的数据量大概有多少？每个索引大概有多少个分片？
但是如果你确实没干过，也别虚，我给你说一个基本的版本，你到时候就简单说一下就好了。
- es 生产集群我们部署了 5 台机器，每台机器是 6 核 64G 的，集群总内存是 320G。
- 我们 es 集群的日增量数据大概是 2000 万条，每天日增量数据大概是 500MB，每月增量数据大概是 6 亿，15G。目前系统已经运行了几个月，现在 es 集群里数据总量大概是 100G 左右。
- 目前线上有 5 个索引（这个结合你们自己业务来，看看自己有哪些数据可以放 es 的），每个索引的数据量大概是 20G，所以这个数据量之内，我们每个索引分配的是 8 个 shard，比默认的 5 个 shard 多了 3 个 shard。