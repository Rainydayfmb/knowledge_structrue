<!-- TOC -->

- [1. 海量数据](#1-海量数据)
    - [1.1. 如何从大量的 URL 中找出相同的 URL？](#11-如何从大量的-url-中找出相同的-url)
    - [1.2. 如何从大量数据中找出高频词？](#12-如何从大量数据中找出高频词)
    - [1.3. 如何找出某一天访问百度网站最多的 IP？](#13-如何找出某一天访问百度网站最多的-ip)
    - [1.4. 如何在大量的数据中找出不重复的整数？](#14-如何在大量的数据中找出不重复的整数)
        - [1.4.1. 题目描述](#141-题目描述)
        - [1.4.2. 解答思路](#142-解答思路)
        - [1.4.3. 方法总结](#143-方法总结)
    - [1.5. 如何在大量的数据中判断一个数是否存在](#15-如何在大量的数据中判断一个数是否存在)
        - [1.5.1. 题目描述](#151-题目描述)
        - [1.5.2. 解答思路](#152-解答思路)
        - [1.5.3. 方法总结](#153-方法总结)
    - [1.6. 如何查询最热门的查询串？](#16-如何查询最热门的查询串)
        - [1.6.1. 题目描述](#161-题目描述)
        - [1.6.2. 解答思路](#162-解答思路)
        - [1.6.3. 方法总结](#163-方法总结)
    - [1.7. 如何统计不同电话号码的个数](#17-如何统计不同电话号码的个数)
        - [1.7.1. 题目描述](#171-题目描述)
        - [1.7.2. 解答思路](#172-解答思路)
        - [1.7.3. 方法总结](#173-方法总结)
    - [1.8. 如何从 5 亿个数中找出中位数？](#18-如何从-5-亿个数中找出中位数)
        - [1.8.1. 题目描述](#181-题目描述)
        - [1.8.2. 解答思路](#182-解答思路)
    - [1.9. 如何按照 query 的频度排序？](#19-如何按照-query-的频度排序)
        - [1.9.1. 题目描述](#191-题目描述)
        - [1.9.2. 解答思路](#192-解答思路)
        - [1.9.3. 方法总结](#193-方法总结)
    - [1.10. 如何找出排名前 500 的数？](#110-如何找出排名前-500-的数)
        - [1.10.1. 题目描述](#1101-题目描述)
        - [1.10.2. 解答思路](#1102-解答思路)

<!-- /TOC -->
# 1. 海量数据
## 1.1. 如何从大量的 URL 中找出相同的 URL？
**题目描述**
给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，内存限制是 4G。请找出 a、b 两个文件共同的 URL。
**解答思路**
每个 URL 占 64B，那么 50 亿个 URL占用的空间大小约为 320GB。
```java
5,000,000,000 * 64B ≈ 5GB * 64 = 320GB
```
由于内存大小只有 4G，因此，我们不可能一次性把所有 URL 加载到内存中处理。对于这种类型的题目，一般采用分治策略，即：把一个文件中的 URL 按照某个特征划分为多个小文件，使得每个小文件大小不超过 4G，这样就可以把这个小文件读到内存中进行处理了。
**思路如下**
首先遍历文件 a，对遍历到的 URL 求 hash(URL) % 1000，根据计算结果把遍历到的 URL 存储到 a0, a1, a2, ..., a999，这样每个大小约为 300MB。使用同样的方法遍历文件 b，把文件 b 中的 URL 分别存储到文件 b0, b1, b2, ..., b999 中。这样处理过后，所有可能相同的 URL 都在对应的小文件中，即 a0 对应 b0, ..., a999 对应 b999，不对应的小文件不可能有相同的 URL。那么接下来，我们只需要求出这 1000 对小文件中相同的 URL 就好了。

接着遍历 ai( i∈[0,999])，把 URL 存储到一个 HashSet 集合中。然后遍历 bi 中每个 URL，看在 HashSet 集合中是否存在，若存在，说明这就是共同的 URL，可以把这个 URL 保存到一个单独的文件中。

**方法总结**
1.分而治之，进行哈希取余；
2.对每个子文件进行 HashSet 统计。

## 1.2. 如何从大量数据中找出高频词？
**题目描述**
有一个 1GB 大小的文件，文件里每一行是一个词，每个词的大小不超过 16B，内存大小限制是 1MB，要求返回频数最高的 100 个词(Top 100)。
**解答思路**
由于内存限制，我们依然无法直接将大文件的所有词一次读到内存中。因此，同样可以采用分治策略，把一个大文件分解成多个小文件，保证每个文件的大小小于 1MB，进而直接将单个小文件读取到内存中进行处理。
**思路如下**：
1.首先遍历大文件，对遍历到的每个词x，执行 hash(x) % 5000，将结果为 i 的词存放到文件 ai 中。遍历结束后，我们可以得到 5000 个小文件。每个小文件的大小为 200KB 左右。如果有的小文件大小仍然超过 1MB，则采用同样的方式继续进行分解。
2.接着统计每个小文件中出现频数最高的 100 个词。最简单的方式是使用 HashMap 来实现。其中 key 为词，value 为该词出现的频率。具体方法是：对于遍历到的词 x，如果在 map 中不存在，则执行 map.put(x, 1)；若存在，则执行 map.put(x, map.get(x)+1)，将该词频数加 1。
3.上面我们统计了每个小文件单词出现的频数。接下来，我们可以通过维护一个小顶堆来找出所有词中出现频数最高的 100 个。具体方法是：依次遍历每个小文件，构建一个小顶堆，堆大小为 100。如果遍历到的词的出现次数大于堆顶词的出现次数，则用新词替换堆顶的词，然后重新调整为小顶堆，遍历结束后，小顶堆上的词就是出现频数最高的 100 个词。
**方法总结**
方法总结：
1.分而治之，进行哈希取余；
2.使用 HashMap 统计频数；
3.求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。
*小顶堆和大顶堆*

## 1.3. 如何找出某一天访问百度网站最多的 IP？
**题目描述**
现有海量日志数据保存在一个超大文件中，该文件无法直接读入内存，要求从中提取某天访问百度次数最多的那个 IP。
**解答思路**
这道题只关心某一天访问百度最多的 IP，因此，可以首先对文件进行一次遍历，把这一天访问百度 IP 的相关信息记录到一个单独的大文件中。接下来采用的方法与上一题一样，大致就是先对 IP 进行哈希映射，接着使用 HashMap 统计重复 IP 的次数，最后计算出重复次数最多的 IP。
*注：这里只需要找出出现次数最多的 IP，可以不必使用堆，直接用一个变量 max 即可*。
**方法总结**
1.分而治之，进行哈希取余；
2.使用 HashMap 统计频数；
3.求解最大的 TopN 个，用小顶堆；求解最小的 TopN 个，用大顶堆。

## 1.4. 如何在大量的数据中找出不重复的整数？
### 1.4.1. 题目描述
在 2.5 亿个整数中找出不重复的整数。注意：内存不足以容纳这 2.5 亿个整数。
### 1.4.2. 解答思路
方法一：分治法
与前面的题目方法类似，先将 2.5 亿个数划分到多个小文件，用 HashSet/HashMap 找出每个小文件中不重复的整数，再合并每个子结果，即为最终结果。

方法二：位图法
对于整数相关的算法的求解，位图法是一种非常实用的算法。假设 int 整数占用 4B，即 32bit，那么我们可以表示的整数的个数为 232。

那么对于这道题，我们用 2 个 bit 来表示各个数字的状态：
- 00 表示这个数字没出现过；
- 01 表示这个数字出现过一次（即为题目所找的不重复整数）；
- 10 表示这个数字出现了多次。

那么这 232 个整数，总共所需内存为 232*2b=1GB。因此，当可用内存超过 1GB 时，可以采用位图法。假设内存满足位图法需求，进行下面的操作：

遍历 2.5 亿个整数，查看位图中对应的位，如果是 00，则变为 01，如果是 01 则变为 10，如果是 10 则保持不变。遍历结束后，查看位图，把对应位是 01 的整数输出即可。

### 1.4.3. 方法总结
判断数字是否重复的问题，位图法是一种非常高效的方法。

## 1.5. 如何在大量的数据中判断一个数是否存在
### 1.5.1. 题目描述
给定 40 亿个不重复的没排过序的 unsigned int 型整数，然后再给定一个数，如何快速判断这个数是否在这 40 亿个整数当中？

### 1.5.2. 解答思路
方法一：分治法
依然可以用分治法解决，方法与前面类似，就不再次赘述了。
方法二：位图法
40 亿个不重复整数，我们用 40 亿个 bit 来表示，初始位均为 0，那么总共需要内存：4,000,000,000b≈512M。
我们读取这 40 亿个整数，将对应的 bit 设置为 1。接着读取要查询的数，查看相应位是否为 1，如果为 1 表示存在，如果为 0 表示不存在。

### 1.5.3. 方法总结
判断数字是否存在、判断数字是否重复的问题，位图法是一种非常高效的方法。

## 1.6. 如何查询最热门的查询串？
### 1.6.1. 题目描述
搜索引擎会通过日志文件把用户每次检索使用的所有查询串都记录下来，每个查询床的长度不超过 255 字节。
假设目前有 1000w 个记录（这些查询串的重复度比较高，虽然总数是 1000w，但如果除去重复后，则不超过 300w 个）。请统计最热门的 10 个查询串，要求使用的内存不能超过 1G。（一个查询串的重复度越高，说明查询它的用户越多，也就越热门。）

### 1.6.2. 解答思路
每个查询串最长为 255B，1000w 个串需要占用 约 2.55G 内存，因此，我们无法将所有字符串全部读入到内存中处理。

方法一：分治法

分治法依然是一个非常实用的方法。
划分为多个小文件，保证单个小文件中的字符串能被直接加载到内存中处理，然后求出每个文件中出现次数最多的 10 个字符串；最后通过一个小顶堆统计出所有文件中出现最多的 10 个字符串。
方法可行，但不是最好，下面介绍其他方法

方法二：HashMap 法
首先，遍历字符串，若不在 map 中，直接存入 map，value 记为 1；若在 map 中，则把对应的 value 加 1，这一步时间复杂度 O(N)。
接着遍历 map，构建一个 10 个元素的小顶堆，若遍历到的字符串的出现次数大于堆顶字符串的出现次数，则进行替换，并将堆调整为小顶堆。
遍历结束后，堆中 10 个字符串就是出现次数最多的字符串。这一步时间复杂度 O(Nlog10)。

方法三：前缀树法
方法二使用了 HashMap 来统计次数，当这些字符串有大量相同前缀时，可以考虑使用前缀树来统计字符串出现的次数，树的结点保存字符串出现次数，0 表示没有出现。

思路如下：
在遍历字符串时，在前缀树中查找，如果找到，则把结点中保存的字符串次数加 1，否则为这个字符串构建新结点，构建完成后把叶子结点中字符串的出现次数置为 1。

最后依然使用小顶堆来对字符串的出现次数进行排序。

### 1.6.3. 方法总结
前缀树经常被用来统计字符串的出现次数。它的另外一个大的用途是字符串查找，判断是否有重复的字符串等。

## 1.7. 如何统计不同电话号码的个数
### 1.7.1. 题目描述
已知某个文件内包含一些电话号码，每个号码为 8 位数字，统计不同号码的个数。

### 1.7.2. 解答思路
这道题本质还是求解数据重复的问题，对于这类问题，一般首先考虑位图法。

对于本题，8 位电话号码可以表示的号码个数为 108 个，即 1 亿个。我们每个号码用一个 bit 来表示，则总共需要 1 亿个 bit，内存占用约 100M。

思路如下：
申请一个位图数组，长度为 1 亿，初始化为 0。然后遍历所有电话号码，把号码对应的位图中的位置置为 1。遍历完成后，如果 bit 为 1，则表示这个电话号码在文件中存在，否则不存在。bit 值为 1 的数量即为 不同电话号码的个数。

### 1.7.3. 方法总结
求解数据重复问题，记得考虑位图法。

## 1.8. 如何从 5 亿个数中找出中位数？
### 1.8.1. 题目描述
从 5 亿个数中找出中位数。数据排序后，位置在最中间的数就是中位数。当样本数为奇数时，中位数为 第 (N+1)/2 个数；当样本数为偶数时，中位数为 第 N/2 个数与第 1+N/2 个数的均值。

### 1.8.2. 解答思路



## 1.9. 如何按照 query 的频度排序？
### 1.9.1. 题目描述
有 10 个文件，每个文件大小为 1G，每个文件的每一行存放的都是用户的 query，每个文件的 query 都可能重复。要求按照 query 的频度排序。

### 1.9.2. 解答思路
如果 query 的重复度比较大，可以考虑一次性把所有 query 读入内存中处理；如果 query 的重复率不高，那么可用内存不足以容纳所有的 query，这时候就需要采用分治法或其他的方法来解决。
方法一：HashMap 法
如果 query 重复率高，说明不同 query 总数比较小，可以考虑把所有的 query 都加载到内存中的 HashMap 中。接着就可以按照 query 出现的次数进行排序。

方法二：分治法

分治法需要根据数据量大小以及可用内存的大小来确定问题划分的规模。对于这道题，可以顺序遍历 10 个文件中的 query，通过 Hash 函数 hash(query) % 10 把这些 query 划分到 10 个小文件中。之后对每个小文件使用 HashMap 统计 query 出现次数，根据次数排序并写入到零外一个单独文件中。

接着对所有文件按照 query 的次数进行排序，这里可以使用归并排序（由于无法把所有 query 都读入内存，因此需要使用外排序）。

### 1.9.3. 方法总结
- 内存若够，直接读入进行排序；
- 内存不够，先划分为小文件，小文件排好序后，整理使用外排序进行归并。

## 1.10. 如何找出排名前 500 的数？
### 1.10.1. 题目描述
有 20 个数组，每个数组有 500 个元素，并且有序排列。如何在这 20*500 个数中找出前 500 的数？

### 1.10.2. 解答思路
对于 TopK 问题，最常用的方法是使用堆排序。对本题而言，假设数组降序排列，可以采用以下方法：

首先建立大顶堆，堆的大小为数组的个数，即为 20，把每个数组最大的值存到堆中。

接着删除堆顶元素，保存到另一个大小为 500 的数组中，然后向大顶堆插入删除的元素所在数组的下一个元素。

重复上面的步骤，直到删除完第 500 个元素，也即找出了最大的前 500 个数。